特征处理

1. one-hot编码：

   > **为什么要离散化？**
   >
   > 连续值的离散化可以提高模型的非线性能力：连续值的离散化可以将一个特征变为多个特征，而每个新特征都有一个权重，这样本来使用一个权重参数管理一个特征变为多个权重参数管理多个特征。至于有多少个特征需要看离散的程度。
   >
   > 例如将(0,1)区间进行离散化，可以离散成(0-0.3)、(0.3-0.6)、(0.6-1)三个区间，这样原始特征中的任意一个值，都可以用这三个特征来表示，特征落在那个区间，那个区间为1，未落在区间里时为0.例如0.2 one-hot编码后为100，0.5 one-hot编码后为010。所以可以使用三个权重值来管理这三个离散后的特征编码，增加了模型的非线性能力。
   >
   > 另外一个好处是：不需要对特征进行归一化，可以加速参数的更新速度。
   >
   > 再者使得一个很大权重值管理一个特征，拆分成了许多小的权重值管理这个特征的多种表示，降低了特征值扰动对模型稳定性影响，也减低了异常值对模型的影响，是模型具有更好的鲁棒性
   >
   > **为什么要映射到欧氏空间？**
   >
   > 应为大部分算法都是基于向量空间中的度量来计算的，在 `回归` 、`分类` 、`聚类` 等机器学习算法中特征之间距离的计算或相似度的计算是非常重要的。
   >
   > **什么是one-hot编码?**
   >
   > 先看一个例子：['male'、‘female'] 使用数字表示效率会很高->[0,1], 但是有一个问题就是分类器往往认为数据是连续的，也就是说分类器认为'male' 比 'female'小，但实际他们并没有大小之分。那么one-hot编码就是：对每一个特征的m个可能值，经过编码后就变成了m个二元特征。并且这些特征互斥，每次只能激活一个，因此数据变得稀松。这样做的优点：
   >
   > 1. 解决了分类器不好处理属性数据的问题
   >
   > 2. 在一定程度上起到了扩充特征的作用(稀疏性)
   >
   > 3. 它的值只有0和1，不同的类型存储在垂直的空间
   >
   >    但当类别数量较多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度，而one-hot+PCA这种组合在实际中也非常有用
   >
   > **为什么要one-hot编码？**
   >
   >  为了使得非偏序关系的变量取值不具有偏序性，并且到原点是等距的。使用one-hot编码，将离散特征的取值扩展到欧式空间，离散特征的取值对应欧式空间中的一个点，也让特征之间的距离计算更加合理
   >
   > **哑编码**
   >
   > 和one-hot编码思想相同，只不过少了一位编码状态。举个例子one-hot编码为[0,0,0,0,1]那么对应的哑编码就是[0,0,0,0]，用四位都没有激活来表达第五位激活了，反之，如果这四位中有一位激活了，那么第五位就是未激活状态喽。

2. 特征预处理

   > 连续特征归一化：线性缩放到[-1,1]，标准化到均值为0，方差为1
   >
   > 离散特征: one-hot编码
   >
   > 有些情况不需要进行特征归一化，如随机森林，bagging和boosting等基于树的方法。
   >
   > 基于参数和距离的模型都需要归一化