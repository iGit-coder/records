激活函数

1. 激活函数有：`sigmoid` `tanh` `relu` `elu  ` `prelu`  `softmax` 

   `sigmoid` : 处处连续便于求导，可以压缩数据，且幅度不变，便于向前传输；在趋向无穷的地方，函数值变化小，容易缺失梯度，不利于神经网络的反馈传输，幂函数还是比较难算。
   $$
   f(x) = \frac{1}{1+e^{-x}}
   $$
   `relu` :克服了梯度消失，加快训练速度,但输入负数则为完全不激活状态，不是以0为中心的函数
   $$
   f(x) = max(0,x)
   $$
   `elu` : 输入负数有一定的输出，并且这部分输入还有一定的抗干扰能力。可以消除`relu` 的死掉问题，不过还是有梯度饱和和指数运算的问题
   $$
   f(x) = \begin{cases}
   x, &x > 0\\
   \alpha (e^x -1),& x\leq 0
   \end{cases}
   $$
   `prelu` : 也是对`relu` 的优化，在负数域内，`prelu` 有一个小的斜率，这样也可以避免`relu` 的死掉问题。相比于`elu` 、`prelu` 在负数域内是线性运算，斜率虽然小，但是不会趋于0
   $$
   f(x) = max(ax,x)
   $$

​        `tanh` : 相比于`sigmoid`， 收敛速度快
$$
f(x) = \frac{1-e^{-2x}}{1+e^{-2x}}
$$
​		`softmax`:  （多分类）
$$
S_j = \frac{e^{a_j}}{\sum_{k=1}^{T}e^{a_k}}
$$
​		`T` ：表示种类数；a<sub>j</sub> :代表输入向量中的第j个分量，这个值是在实数范围。S<sub>j</sub>: 代表输入向量第j个分量为种类j的概率,【认为输入向量第j个分量代表第j个种类概率，然后进行优化】，`j`的取值范围为`1-T` .例如三分类问题，种类为1,2,3，输入为[9,4,3],经过softmax之后为[0.99,0.007,0.003],则表示此次输出认为种类为1的概率最大，但真实的种类为2，需要之后进行优化，使用交叉熵损失函数进行训练。

