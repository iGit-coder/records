### 优化器理论

优化器: 基于梯度的最优解计算方法，有传统的梯度下降`GD(BGD)` 、随机梯度下降`SGD` 、`SGD+Momentum` 、`Nesterov` 、`AdaGrad` 、`RMSprop`、 `AdaDelta`   `Adam`

`GD` ：梯度下降是最原始的也是最基础的算法。

![](E:\md\resources\优化器2.png)

它将所有的数据集都载入，计算他们所有的梯度，然后执行决策。即**一定**要沿着**梯度相反**的方向更新权重。

优点是：凸函数能收敛到最小值。但显而易见是，计算量太大。容易收敛到局部最小值。

`SGD` : 随机梯度下降相比于`GD` ，其实就是计算梯度时根据的数据不同而已。由于`SGB` 是从整个数据集中随机取一部分进行梯度计算，所以在相同时间内参数更新较频繁，自然收敛的更快。但是，根据海森矩阵，假设二维的例子中损失函数在某个方向变化很快但在另外个方向变化很慢，`SGD`就会在变化慢的方向来回，导致变化快的方向不会有很大的跨度。由于使用部分数据，噪声会相对较大，但是噪声同时可以帮助`SGD` 逃出鞍点(局部最优点)、

![image-20201105153848011](E:\md\resources\SGD.png)

`SGD+Momentum` ：

![image-20201105153946074](E:\md\resources\SGD+Momentum.png)
$$
\begin{align}
&SGD:\\
x_{t+1} &= x_{t} - \alpha \nabla f(x_t)\\
----&---------\\
SGD+&Momentum:\\
v_{t+1} &= pv_{t} + \nabla f(x_{t})\\
x_{t+1} &= x_t - \alpha v_{t+1}
\end{align}
$$
可以看出，`SGD` 更新的梯度不仅是算出来的梯度，更要加上上一步的梯度。但有个参数`p`让我们调整上一步梯度占的比重。 可以解决 `SGD` 海森(Hessian)矩阵病态问题【可以理解为`SGD` 在收敛过程中和正确梯度相比来回摆动比较大的问题】。可以形象地理解为：当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动时带上了惯性。这样可以加快小球向下滚动的速度。

`NAG` ：牛顿加速梯度(Nesterov accelerated gradient) 算法，是Momentum动量算法的变种
$$
\begin{align}
Momentum: g(b) &= pg(a) + g(b)\\
g(b)\prime &= b - \alpha g(b)\\
-------&--------\\
Nesterov: g(b) &= pg(a) + g(b - \alpha \cdot pg(a))\\
			&=pg(a) + g(c) \\ 
			g(b)\prime &= b - \alpha \cdot g(b)
\end{align}\\
$$

- Nesterov动量梯度的计算在模型参数施加当前速度之后，因此可以理解为往标准动量中添加了一个矫正因子。
- 理解策略：在Momentum中小球会盲目地跟从下坡的梯度，容易发生错误。所以需要一个更聪明的小球，能提前知道它要去哪里，还要知道走到坡底的时候速度慢下来而不是又冲上另外一个坡。计算 `b-apg(a)` 可以表示小球下一个位置大概在哪里。从而可以提前知道下一个位置的梯度，然后使用到当前位置来更新参数。
- 在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从O(1/k)(k步后)改进到O(1/k<sup>2</sup>) 。然而，在随机情况下，Nesterov动量收敛率的作用却不是很大。 

`AdaGrad` : 累计每一次梯度的平方，接着让学习率除以它的开方grad_squared。这个作用就是为了改变不同参数的学习率。假如一个参数的梯度一直很大，那么通过这个约束，它改变的就越少。加入一个参数的梯度一直很小，通过这个约束，它变化的也就越快。`AdaGrad` 对出现较多的类别数据，给予越来越小的学习率，而对于比较少的类别数据，给予较大的学习率。因此使用于数据稀疏或者分布不均衡的数据集。`AdaGrad` 的优势主要是不用认为地调节学习率，他可以自动调节；

但问题是，梯度一直在积累，一定会越来越大，最后的结果是，权重更新的步长也会不可避免地变小。为了克服这个问题，有了RMSpro

![image-20201105172533341](E:\md\resources\AdaGrad.png)

`RMSProp` 把`AdaGrad` 的梯度累计改为指数加权的移动平均，【即添加了一个衰减率(一般0.9或者0.99】，可以使得grad_squared的变化不会因为时间的累计而变得太大。而且可以自适应调节学习率。在经验上已经被证明是一种有效且使用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。

但是由于衰减的问题，grad_squared可能导致我们训练一直在变慢的。(后面积累的梯度太少了，但因为前面的梯度太多，没有办法加速)

![image-20201105172605714](E:\md\resources\RMSProp.png)

`Adam` ：结合了Momentum和 RMSProp,并且可以避免一个问题：一开始的梯度太小，因此学习率除以second_moment的值太大，步长太大容易跑到一个奇怪的地方。导致收敛之后效果不好。因此我们有了Bias correction, 学习率也是除以second_unbias. `Adam` 一般来说是收敛最快的优化器，所以被用的比较频繁。当然，他也有变体 `AdamMax` 

`AdaDelta` 算法思想：`AdaDelta` 算法和 `RMSProp` 算法都需要指定全局学习率，而 `AdaDelta` 不需要设置一个默认的全局学习率。在模型训练的初期和中期，`AdaDelta` 表现很好，加速效果不错，训练速度快。在模型训练后期，模型会反复地在局部最小值附近抖动。

![image-20201105172637302](E:\md\resources\Adam.png)

### 各个优化器效果图

<img src="E:\md\resources\优化器效果图.gif" style="zoom: 67%;" /> <img src="E:\md\resources\优化器效果图0.gif" style="zoom: 80%;" />

<img src="E:\md\resources\优化器1.gif"  />